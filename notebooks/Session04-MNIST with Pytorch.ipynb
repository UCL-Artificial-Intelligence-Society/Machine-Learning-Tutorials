{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Perceptrons\n",
    "2. Digit recognition using Tensorflow\n",
    "3. **Digit recognition using Pytorch**\n",
    "\n",
    "### Aim\n",
    "At the end of this session, you will be able to:\n",
    "- understand perceptrons in the context of logistic regression\n",
    "- understand feed-forward ANNs\n",
    "- feel the differences of the two most famous DL frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load relevant data\n",
    "\n",
    "Like in the Keras\\Tensorflow notebook, we'll be using MNIST, a dataset of 28x28 grayscale images of hand-written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we load the images, they will be in Image objects\n",
    "# We want to convert them to PyTorch tensors, which are similar to NumPy arrays\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "# When we load the datasets for the first time, we download them from the internet\n",
    "# In later instances, we can access the downloaded files locally\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "# The DataLoader defines the mini-batch size (which is often confusingly called batch_size)\n",
    "# and whether to shuffle our dataset when splitting into batches.\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "# TODO: Make a testloader that doesn't shuffle the data\n",
    "testloader = None\n",
    "\n",
    "# Our classes are just the digits from 0 to 9\n",
    "classes = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get some random training images this way\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    # The make_grid PyTorch function returns tensor with re-ordered\n",
    "    # dimensions, so we need to manually fix that.\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# PyTorch has a lot of utilities for visualisations and pre-processing\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "If we run our data through only the PyTorch functions (ones in the `nn` and `F` modules below), the framework can automatically calculate gradients for us. Our job here is just to define the number of layers and number of hidden units per layer. We have to make sure that our final number of units matches the number of classes (10 for the case of MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# We want a network with 2 hidden layers of 64 and 32 units respectively\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # The nn.Linear function defines a fully-connected (affine) layer\n",
    "        # TODO: Fill in the number of units specified above. Note that each input should be the same\n",
    "        # as the preceeding output\n",
    "        self.fc1 = nn.Linear(28 * 28, None)\n",
    "        self.fc2 = nn.Linear(None, None)\n",
    "        self.fc3 = nn.Linear(None, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We want to treat our image as a flattened array of 784 pixels, so we reshape it\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # We apply the ReLU non-linearity for each hidden layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # TODO: What should this be?\n",
    "        x = F.None(self.None(x))\n",
    "        # We don't use a ReLU for the final layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise the weights\n",
    "\n",
    "Once we've defined our model, we need to choose the appropraite loss and optimiser. For classification, we most often use the `CrossEntropyLoss`. You can play around with the optimizer and see what its effect is on the performance of the model, but your default choice will usually be stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# The learning rate (lr) was discussed in the pre-recorded lectures.\n",
    "# TODO: Make the lr 0.0000001 and 10.0 and re-run the below. What happens? Can you think of why that happens?\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# We call one run over the entire dataset an epoch. More complex models require more epochs to train,\n",
    "# but for this example 2 epochs should be sufficient.\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    # Each epoch runs over the entire dataset. PyTorch prefers to do this with iterators\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # data will be a list of batch_size pairs of images and labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # *Important*: We have to manually set the parameter gradients to zero ourselves\n",
    "        # Missing this is one of the most common mistakes when using PyTorch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # We get predictions with the network by calling it this way\n",
    "        # outputs will be B vectors of 10 numbers called logits\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # The loss function takes the logits and converts them to probabilities.\n",
    "        # It then compares those probabilities to the one-hot encoded versions of the labels\n",
    "        # E.g. 7 is converted to [0 0 0 0 0 0 0 1 0 0]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # This function computes the gradients for the current batch\n",
    "        loss.backward()\n",
    "        \n",
    "        # The optimiser then uses the gradients to change the parameters with the equation discussed\n",
    "        # in the live lecture.\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load the model\n",
    "\n",
    "PyTorch allows us to save and load the model just by specifying the place we want the model to live. It takes a couple of lines to do either operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './mnist_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_net = Net()\n",
    "# TODO: What should the argument be?\n",
    "loaded_net.load_state_dict(torch.load(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using the trained model\n",
    "\n",
    "Once we have trained our model or loaded a pre-trained one, prediction looks exactly like it did during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the first 4 images from the test set (check you understand why we don't get random images here)\n",
    "dataiter = iter(testloader)\n",
    "images, labels = None\n",
    "\n",
    "# Print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "# Get the logits of our network\n",
    "# The no_grad function tells PyTorch not to compute gradients in the scope, which\n",
    "# can save a lot of memory and computation. Whenever you're testing you should use this.\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_net(images)\n",
    "\n",
    "# Each prediction is 10 logits, so we take the index for which the logits is highest\n",
    "# Warning: torch.max returns both the max and the argmax! We want the argmax, so we take\n",
    "# the second object returned\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to compute some statistics over the entire testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# We first compute the overall accuracy\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # TODO: Get logits from our network\n",
    "        outputs = None\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# TODO: Compute the accuracy per class\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = loaded_net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # TODO: Get correct and total per class\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: How to train on GPU using PyTorch\n",
    "\n",
    "Assuming that we are on a machine with CUDA capabilities (this can be sometimes difficult to set up on your own machine, but some uni servers have them), the below code should print a CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making PyTorch run on the GPU is then as easy as calling the below functions before doing any computation on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)\n",
    "inputs, labels = data[0].to(device), data[1].to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
