{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we use sklearn??\n",
    "\n",
    "1. **Example Datasets**  \n",
    "    sklearn.datasets : Provides example datasets  \n",
    "\n",
    "2. **Feature Engineering**  \n",
    "    sklearn.preprocessing : Variable functions as to data preprocessing  \n",
    "    sklearn.feature_selection : Help selecting primary components in datasets  \n",
    "    sklearn.feature_extraction : Vectorised feature extraction  \n",
    "    sklearn.decomposition : Algorithms regarding Dimensionality Reduction  \n",
    "\n",
    "3. **Data split and Parameter Tuning**  \n",
    "    sklearn.model_selection : 'Train Test Split' for cross validation, Parameter tuning with GridSearch  \n",
    "\n",
    "4. **Evaluation**  \n",
    "    sklearn.metrics : accuracy score, ROC curve, F1 score, etc.  \n",
    "\n",
    "5. **ML Algorithms**  \n",
    "    sklearn.ensemble : Ensemble, etc.  \n",
    "    sklearn.linear_model : Linear Regression, Logistic Regression, etc.  \n",
    "    sklearn.naive_bayes : Gaussian Naive Bayes classification, etc.  \n",
    "    sklearn.neighbors : Nearest Centroid classification, etc.  \n",
    "    sklearn.svm : Support Vector Machine  \n",
    "    sklearn.tree : DecisionTreeClassifier, etc.  \n",
    "    sklearn.cluster : Clustering (Unsupervised Learning)  \n",
    "    \n",
    "6. **Utilities**  \n",
    "    sklearn.pipeline: pipeline of (feature engineering -> ML Algorithms -> Prediction)  \n",
    "    \n",
    "7. **Train and Predict**  \n",
    "    fit()  \n",
    "    predict()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/__init__.py:1003: UserWarning: Duplicate key in file \"/Users/toeun_kim/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris data: \n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "iris target: \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "data length:  150\n",
      "target length:  150\n",
      "feature names: \n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "target names: \n",
      " ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# TODO: load iris data (famous classification dataset), get data and label(target)\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"iris data: \\n\", iris.data[:5])\n",
    "print(\"iris target: \\n\", iris.target)\n",
    "\n",
    "print(\"data length: \", len(iris.data))\n",
    "print(\"target length: \", len(iris.target))\n",
    "\n",
    "print(\"feature names: \\n\", iris.feature_names)\n",
    "print(\"target names: \\n\", iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "5                5.4               3.9                1.7               0.4\n",
      "6                4.6               3.4                1.4               0.3\n",
      "7                5.0               3.4                1.5               0.2\n",
      "8                4.4               2.9                1.4               0.2\n",
      "9                4.9               3.1                1.5               0.1 \n",
      "\n",
      "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
      "count         150.000000        150.000000         150.000000   \n",
      "mean            5.843333          3.057333           3.758000   \n",
      "std             0.828066          0.435866           1.765298   \n",
      "min             4.300000          2.000000           1.000000   \n",
      "25%             5.100000          2.800000           1.600000   \n",
      "50%             5.800000          3.000000           4.350000   \n",
      "75%             6.400000          3.300000           5.100000   \n",
      "max             7.900000          4.400000           6.900000   \n",
      "\n",
      "       petal width (cm)  \n",
      "count        150.000000  \n",
      "mean           1.199333  \n",
      "std            0.762238  \n",
      "min            0.100000  \n",
      "25%            0.300000  \n",
      "50%            1.300000  \n",
      "75%            1.800000  \n",
      "max            2.500000   \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      "sepal length (cm)    150 non-null float64\n",
      "sepal width (cm)     150 non-null float64\n",
      "petal length (cm)    150 non-null float64\n",
      "petal width (cm)     150 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 4.8 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO: Homework -> Draw meaningful graphs using pandas and matplotlib\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "print(data.head(10), '\\n')\n",
    "print(data.describe(), '\\n')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load classifier model\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train without splitting data\n",
    "model.fit(iris.data, iris.target)\n",
    "\n",
    "# TODO: predict targets based on your x datasets\n",
    "pred = model.predict(iris.data)\n",
    "\n",
    "# TODO: Evaluate your prediction by comparing it with label you used for training\n",
    "# You must get 100% Accuracy!\n",
    "print(\"Accuracy : {}\".format(accuracy_score(iris.target, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Too Good to be true...**  \n",
    "You are actually testing your model's prediction ability based on the data that you have already used for training. \n",
    "Unless you have an unused dataset explicitly for testing, you need to split the data into training purpose data and testing purpose data.  \n",
    "This is where [**sklearn.model_selection.train_test_split**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) comes into play.  \n",
    "\n",
    "train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)\n",
    "\n",
    "- \\*arrays : x and y data  \n",
    "- test_size : Ratio of Test data (default = 0.25)  \n",
    "- train_size : Ratio of Train data (default = 1 - 0.25)  \n",
    "- random_state : seed value for shuffle. It is used to seed a new RandomState object. This is to check and validate the data when running the code multiple times  \n",
    "- shuffle : shuffle or not? (default = True)  \n",
    "- stratify : will discuss later on (default = None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 112\n",
      "Length of X_test: 38\n",
      "Length of Y_train: 112\n",
      "Length of Y_test: 38\n",
      "train_test ratio: 0.25%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Split your data into X_train, X_test, Y_train and Y_test\n",
    "# TODO: What is optimal rate for the test_size?\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"Length of X_train: {}\".format(len(X_train)))\n",
    "print(\"Length of X_test: {}\".format(len(X_test)))\n",
    "print(\"Length of Y_train: {}\".format(len(Y_train)))\n",
    "print(\"Length of Y_test: {}\".format(len(Y_test)))\n",
    "print(\"train_test ratio: {0:.2f}%\".format(\n",
    "    len(X_test) / (len(X_train)+len(X_test))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "# TODO: Train your model with your 'train data' not the whole data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# TODO: Predict targets with your 'test data' not the whole target data\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# TODO: Evaluate your prediction. Which data should you compare your predicted data with?\n",
    "print(\"Accuracy : {}\".format(accuracy_score(Y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Fold Cross Validation\n",
    "\n",
    "Visit [KFold official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)  \n",
    "\n",
    "Cross Validation is encouraged to use when dataset is not big enough.  \n",
    "Cross Validation is used to avoid overfitting.  \n",
    "Overfitting literally means 'data is overly fitted with the data' so that it does not perform well when new data are given.  \n",
    "K-fold Cross Validation makes K number of train and test data set.  \n",
    "\n",
    "**KFold(n_splits)**  \n",
    "```\n",
    "n_splits : the number of folds (splits)  \n",
    "KFold(n_splits=N).split(X)  \n",
    "X : Data  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train index: \n",
      " [ 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47\n",
      "  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "train index shape:  (120,)\n",
      "test index: \n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "test index shape:  (30,)\n",
      "Iteration : 1, Cross-Validation Accuracy : 1.0\n",
      "\n",
      "\n",
      "train index: \n",
      " [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "train index shape:  (120,)\n",
      "test index: \n",
      " [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
      " 54 55 56 57 58 59]\n",
      "test index shape:  (30,)\n",
      "Iteration : 2, Cross-Validation Accuracy : 0.967\n",
      "\n",
      "\n",
      "train index: \n",
      " [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  90  91  92  93  94  95  96  97  98  99 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "train index shape:  (120,)\n",
      "test index: \n",
      " [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
      " 84 85 86 87 88 89]\n",
      "test index shape:  (30,)\n",
      "Iteration : 3, Cross-Validation Accuracy : 0.867\n",
      "\n",
      "\n",
      "train index: \n",
      " [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "train index shape:  (120,)\n",
      "test index: \n",
      " [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "test index shape:  (30,)\n",
      "Iteration : 4, Cross-Validation Accuracy : 0.933\n",
      "\n",
      "\n",
      "train index: \n",
      " [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "train index shape:  (120,)\n",
      "test index: \n",
      " [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "test index shape:  (30,)\n",
      "Iteration : 5, Cross-Validation Accuracy : 0.733\n",
      "\n",
      "\n",
      "Average accuracy :  0.9\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "# TODO: How many folds do you want?\n",
    "# TODO: Try KFold(n_splits=3)\n",
    "n_iter = 1      # this var is made just to keep track of the number of iteration\n",
    "kfold = KFold(n_splits = 5)\n",
    "cv_accuracy = []\n",
    "\n",
    "# type of idx => numpy ndarray\n",
    "for train_idx, test_idx in kfold.split(iris.data):\n",
    "    print(\"train index: \\n\", train_idx)\n",
    "    print(\"train index shape: \", train_idx.shape)\n",
    "    print(\"test index: \\n\", test_idx)\n",
    "    print(\"test index shape: \", test_idx.shape)\n",
    "    X_train, X_test = iris.data[train_idx], iris.data[test_idx]\n",
    "    y_train, y_test = iris.target[train_idx], iris.target[test_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 3)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "\n",
    "    print(\"Iteration : {}, Cross-Validation Accuracy : {}\".format(n_iter, accuracy))\n",
    "\n",
    "    n_iter += 1\n",
    "\n",
    "    cv_accuracy.append(accuracy)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Average accuracy : \", np.mean(cv_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified K-Fold Cross Validation\n",
    "Have you tried KFold(n_splits=3) from the above K-Fold example? What happend? Why does that happen?  \n",
    "Plus, when you do KFold(n_splits=5), how can the accuracy reach 1.0 at the first iteration?\n",
    "**The reason why is because the distribution of the data is UNBALANCED**  \n",
    "**This is where Stratified K-Fold CV comes into play**  \n",
    "\n",
    "Helpful URL to understand this concept:\n",
    "1. [sklearn document](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)  \n",
    "\n",
    "**code from sciket-learn doc**\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])  -->\n",
    "y = np.array([0, 0, 1, 1])                      --> Here you can see that the distribution of the data is unbalanced, where normal KFold is not desirable\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]\n",
    "```\n",
    "\n",
    "2. [Quora](https://www.quora.com/What-is-difference-between-k-fold-and-stratified-k-fold-cross-validations)  \n",
    "3. [StackExchange](https://stats.stackexchange.com/questions/49540/understanding-stratified-cross-validation)\n",
    "\n",
    "\n",
    "Stratified KFold is encouraged to use when the distribution of the given data is unbalanced, where normal KFold creates high bias to your model.\n",
    "\n",
    "\n",
    "StratifiedKFold(n_splits)\n",
    "n_splits : the number of splits i.e. folds\n",
    "\n",
    "StratifiedKFold(n_splits=5).split(X, Y)\n",
    "X : Data\n",
    "Y : label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1\n",
      "--------------------\n",
      "Check distribution of train data : \n",
      " 2    33\n",
      "1    33\n",
      "0    33\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Check distribution of test data : \n",
      " 2    17\n",
      "1    17\n",
      "0    17\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Iteration : 1, Accuracy : 98.04%, Size of Train data : 99, Size of Test data : 51\n",
      "\n",
      "Iteration : 2\n",
      "--------------------\n",
      "Check distribution of train data : \n",
      " 2    33\n",
      "1    33\n",
      "0    33\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Check distribution of test data : \n",
      " 2    17\n",
      "1    17\n",
      "0    17\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Iteration : 2, Accuracy : 92.16%, Size of Train data : 99, Size of Test data : 51\n",
      "\n",
      "Iteration : 3\n",
      "--------------------\n",
      "Check distribution of train data : \n",
      " 2    34\n",
      "1    34\n",
      "0    34\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Check distribution of test data : \n",
      " 2    16\n",
      "1    16\n",
      "0    16\n",
      "Name: label, dtype: int64\n",
      "--------------------\n",
      "Iteration : 3, Accuracy : 100.0%, Size of Train data : 102, Size of Test data : 48\n",
      "\n",
      "Average accuracy : 0.9673333333333334\n"
     ]
    }
   ],
   "source": [
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['label'] = iris.target\n",
    "\n",
    "# Classification model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "n_iter = 0      # this var is for tracking the iteration\n",
    "SKF = StratifiedKFold(n_splits=3)\n",
    "avg_acc = []\n",
    "\n",
    "for train_idx, test_idx in SKF.split(iris.data, iris.target):\n",
    "\n",
    "    n_iter += 1\n",
    "\n",
    "    # TODO: split it and assign data to variables\n",
    "    X_train, X_test = iris.data[train_idx], iris.data[test_idx]\n",
    "    y_train, y_test = iris.target[train_idx], iris.target[test_idx]\n",
    "\n",
    "    print(\"Iteration :\", n_iter)\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    '''\n",
    "    Refer to this site (about Series.value_counts()): \n",
    "    https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html\n",
    "    '''\n",
    "\n",
    "    print(\"Check distribution of train data : \\n\",\n",
    "          iris_df['label'].iloc[train_idx].value_counts())\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    print(\"Check distribution of test data : \\n\",\n",
    "          iris_df['label'].iloc[test_idx].value_counts())\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    # train your model with train data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make your model predict data with test data\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    # TODO: Evaluate your model\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "\n",
    "    print(\"Iteration : {}, Accuracy : {}%, Size of Train data : {}, Size of Test data : {}\\n\"\n",
    "          .format(n_iter, accuracy * 100, train_size, test_size))\n",
    "\n",
    "    avg_acc.append(accuracy)\n",
    "\n",
    "print(\"Average accuracy : {}\".format(np.mean(avg_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train, Validation and Test data set\n",
    "\n",
    "[medium blog](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)\n",
    "\n",
    "**Validation Set**  \n",
    "The validation set is used to evaluate a given model, but this is for frequent evaluation. We as machine learning engineers use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never does it “Learn” from this. We(mostly humans, at-least as of 2017 😛 ) use the validation set results and update higher level hyperparameters. So the validation set in a way affects a model, but indirectly. - from the blog-\n",
    "\n",
    "You split training set into train and validation sets (for tuning),\n",
    "and check its performance with test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train_test_split(), choose good test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "\n",
    "# TODO: declare kfold or SKfold\n",
    "# kfold = KFold(n_splits=3)\n",
    "STK = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# TODO: Split X_train into train and validation set\n",
    "for train_idx, vali_idx in STK.split(X_train, y_train):\n",
    "    X_train, X_vali = iris.data[train_idx], iris.data[vali_idx]\n",
    "    y_train, y_vali = iris.target[train_idx], iris.target[vali_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
