{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCL AI Society Machine Learning Tutorials\n",
    "### Session 02. Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. Train/test set split\n",
    "2. Various Cross Validation teqniques\n",
    "3. Regressions\n",
    "4. Classifications\n",
    "5. All in one challenge: Boston house\n",
    "\n",
    "## Aim\n",
    "At the end of this session, you will be able to:\n",
    "- Understand how to prepare your dataset.\n",
    "- Use the most popular library in Machine Learning\n",
    "- Implement simple regression and classification models.\n",
    "- Practice a whole machine learning project individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction to Classification\n",
    "From the last section, you've learned what Regression is. Classification is another important branch in Machine Learning.  \n",
    "So what is classification?  \n",
    "It is an area of supervised learning that tries to predict which class or category some entity belongs to, based on its features.  \n",
    "\n",
    "Classification can have four types of result:\n",
    "1. True Negative\n",
    "2. Treu Positive\n",
    "3. False Negative\n",
    "4. False Positive\n",
    "We will dive more into this, later in this tutorial series.\n",
    "\n",
    "\n",
    "In this notebook we will discover when we should use classification rather than regression, and types of classification algorithms.\n",
    "\n",
    "Type of Classifiers:\n",
    "- Logistic Regression (Its name is regression, but it's much more close to classification than regression)\n",
    "- Softmax\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "- KNN\n",
    "- Decision Tree\n",
    "\n",
    "This notebook will cover logistic regression and Decision tree. Others can possibly dealt in the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "Useful videos:\n",
    "1. [Andrew Ng's explanation 1](https://www.youtube.com/watch?v=-la3q9d7AKQ)\n",
    "2. [Andrew Ng's explanation 2](https://www.youtube.com/watch?v=t1IT5hZfS48)\n",
    "3. [Andrew Ng's explanation 3](https://www.youtube.com/watch?v=F_VG4LNjZZw)\n",
    "4. [Andrew Ng's explanation 4](https://www.youtube.com/watch?v=HIQlmHxI6-0)\n",
    "\n",
    "Logistic regression is a well-motivated approach to discriminative classification which leads to a smooth, convex, optimisation problem.  \n",
    "\n",
    "Logistic regression is also a basis of Neural Network. Logistic Regression is sometimes called, a single node of Artificial Neuron. We will get back to what this means afterwards when we are doing Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. In which case do we use classification?\n",
    "\n",
    "**Let's generate a toy dataset that is suitable for classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_samples= 100\n",
    "\n",
    "X = np.random.normal(size=n_samples)\n",
    "y = (X > 0).astype(np.float)\n",
    "\n",
    "X[X > 0] *= 5\n",
    "X += .7 * np.random.normal(size=n_samples)\n",
    "X = X[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if our data looks like the above? Would you still use your linear regression model?  \n",
    "Probably not. When your data has classes and your task is to classify the data, you normally use classification method, and Logistic Regression is a good start in learning classification.  \n",
    "Please do watch the Andrew Ng's video on Logistic Regression to fully understand mathematically.  \n",
    "\n",
    "I've written a function called `compare_logistic_linear`, which fits the data into the logistic regression model and a simple ordinary least squared linear regression model. Then, it plots the two in one plot for better visual representation on why you should consider using classification rather than regression.  \n",
    "\n",
    "Plus, note the the term 'logistic regression' has a word 'regression' inside.  \n",
    "It is because the logistic regression is a generalised linear model using the same basic formula of linear regression but it is regressing for the probability of a categorical outcome by using `sigmoid` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_logistic_linear(model, X_data, y_data):\n",
    "    \"\"\"\n",
    "    This function plots the given data - X_data and y_data\n",
    "    then fit the data both into given `model` and LinearRegression model.\n",
    "    Then shows the difference by plotting both of them.\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    plt.scatter(X_data.ravel(), y_data, color='black', zorder=20)\n",
    "    X_test = np.linspace(-5, 10, 300)\n",
    "\n",
    "    loss = expit(X_test * model.coef_ + model.intercept_).ravel()\n",
    "    plt.plot(X_test, loss, color='red', linewidth=3)\n",
    "    \n",
    "    # Ordinary Least Squared Linear Regression\n",
    "    ols = linear_model.LinearRegression()\n",
    "    ols.fit(X_data, y_data)\n",
    "    plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\n",
    "    plt.axhline(.5, color='.5')\n",
    "\n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('X')\n",
    "    plt.xticks(range(-5, 10))\n",
    "    plt.yticks([0, 0.5, 1])\n",
    "    plt.ylim(-.25, 1.25)\n",
    "    plt.xlim(-4, 10)\n",
    "    plt.legend(('Logistic Regression Model', 'Linear Regression Model'),\n",
    "               loc=\"lower right\", fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_clf = linear_model.LogisticRegression(C=1e5, solver='lbfgs')\n",
    "logistic_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_logistic_linear(logistic_clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Implementing a logistic classifier from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "\"\"\"\n",
    "If you are not familiar with this iris dataset, feel free to explore this dataset by EDA. \n",
    "However, for now, I recommend you to just assume that you are given this data.\n",
    "We will explore this dataset in the later section of this notebook (2-3. Decision Tree)\n",
    "\"\"\"\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # We will only use two features. Do not change X.\n",
    "y = (iris.target != 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:3], y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have watched Andrew Ng's lecture video, you will know that the cost function for logistic regression is below:  \n",
    "\\begin{align}\n",
    "J\\left(\\theta \\right) & =  -{\\frac{1}{n}}[\\sum_{i=1}^n \\left(y_i \\log_2(P_r(\\hat{y}=1|x_i;\\theta))+(1-y_i)\\log_2(1-P_r(\\hat{y}=1|x_i;\\theta)) \\right)]\\\\\n",
    "\\end{align}\n",
    "\n",
    "You are asked to complete the LogisticRegression Class below.   \n",
    "y_hat should be:  \n",
    "\\begin{align}\n",
    "\\hat{y} & = \\frac{1}{1+e^{-\\theta^{*t}x}}\n",
    "\\end{align}\n",
    "\n",
    "Since you are already familiar with the Iris dataset which is an in-built dataset provided by scikit learn, we will use that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # TODO\n",
    "    # return y_hat. Look at the cell above. e to the -theta time x is just a z, our input of this function.\n",
    "    return None\n",
    "\n",
    "def loss(h, y):\n",
    "    # TODO:\n",
    "    # return the loss function J\n",
    "    return None\n",
    " \n",
    "def logRegParamEstimates(x_train, y_train):\n",
    "    ## TODO: Fill in the 'None's commented section ##\n",
    "    \n",
    "    intercept = np.ones((\"\"\"What should be the dimension?\"\"\"))\n",
    "    x_train = np.concatenate((intercept, x_train), axis=1)\n",
    "    y_train[y_train > 1] = 0\n",
    "    # Initialisation of theta.\n",
    "    theta = np.zeros(\"\"\"What should be the dimension of theta?\"\"\")\n",
    "    lr = 0.01  # learning rate\n",
    "    for i in range(100):\n",
    "        z = np.dot(\"\"\"dot product of which two?\"\"\")\n",
    "        h = sigmoid(\"\"\"sigmoid of what?\"\"\")\n",
    "        # gradient part below was going to be TODO'ed, but since it can be hard to some of you guys, I'll just leave it.\n",
    "        # But don't overlook this gradient part.\n",
    "        gradient= ( 1 / (math.log(2) * x_train.shape[0]) ) * np.dot( (h-y_train), x_train)\n",
    "        \n",
    "        # TODO: update theta\n",
    "        # you reassign theta as theta minus gradient*learning_rate\n",
    "        # Imagine you are walking down the gradient convex with a step size of `learning rate`*gradient.\n",
    "        # Think why you should subtract the `gradient*learning_rate' rather than adding it to the theta.\n",
    "        theta = None\n",
    "    \n",
    "    return theta\n",
    "\n",
    "def logistic_regression_predict(x_train, y_train, x_test):\n",
    "    ## TODO: Fill in the 'None's  ##\n",
    "    \n",
    "    # Get theta matrix by calling the above function\n",
    "    theta = None\n",
    "    # This can be an hint to the solution somewhere above where you are asked to fill in a dimension.\n",
    "    intercept = np.ones((x_test.shape[0], 1))\n",
    "    x_test = np.concatenate((intercept, x_test), axis=1)\n",
    "    sig = sigmoid(np.dot(x_test, theta))\n",
    "    y_pred1=[]\n",
    "    for i in range(len(sig)):\n",
    "        if sig[i]>=0.5:\n",
    "            y_pred1.append(1)\n",
    "        else:\n",
    "            y_pred1.append(3)\n",
    "    return y_pred1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = logistic_regression_predict(x_train, y_train,x_test)\n",
    "print('Accuracy on test set: '+str(accuracy_score(y_test,y_pred1)))\n",
    "print(classification_report(y_test,y_pred1))#text report showing the main classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision Tree\n",
    "We will now look into different classification algorithm, which is called a Decision Tree Classifer.  \n",
    "Before we dive into it, let's first get to know what tree is in computer science.\n",
    "\n",
    "\n",
    "## 2-1. Getting used to Tree Structure (optional)\n",
    "The most basic tree is a Binary Tree, and it is crucial to know what Binary Tree is to undertand the Decision Tree Classifiers.  \n",
    "There are two search algorithms in tree structure. One is **Breadth First Search (BFS)** and the other is **Depth First Search (DFS)**. BFS and DFS are not only used in tree but also used in general graph structure. However, in this notebook we will only look into the cases in trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  \n",
    "Don't worry if you are not familiar with recursion and data structure. This section is just to show you what Tree is. Just get the gist of the way it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Binary Tree Breath First Search (BFS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFS uses Queue as its Data Structure\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeBinaryTree(lst):\n",
    "    def helper(index):\n",
    "        if index >= len(lst):\n",
    "            return None\n",
    "        node = Node(lst[index])\n",
    "        node.left = helper(index * 2 + 1)     # Think why the index should be 2n+1\n",
    "        node.right = helper(index * 2 + 2)    # Think why the index should be 2n+2\n",
    "        return node\n",
    "\n",
    "    return helper(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is implemented as BFS algorithm.\n",
    "def printTree(node):\n",
    "    all_lines = []\n",
    "    line = []\n",
    "\n",
    "    q = queue.Queue()\n",
    "    q.put(node)\n",
    "    q.put(Node(-1))\n",
    "\n",
    "    while q.qsize() > 0:\n",
    "        node = q.get()\n",
    "\n",
    "        if not node:\n",
    "            continue\n",
    "        else:\n",
    "            if node.val == -1:\n",
    "                if q.qsize() > 0:\n",
    "                    all_lines.append(line)\n",
    "                    line = []\n",
    "                    q.put(Node(-1))\n",
    "            else:\n",
    "                line.append(node.val)\n",
    "                q.put(node.left)\n",
    "                q.put(node.right)\n",
    "\n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different lists and visualise how the tree looks like\n",
    "node_bfs = completeBinaryTree([1, 2, 3, 4, 5, 6, 7])\n",
    "printTree(node_bfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Binary Tree Depth First Search (DFS)** \n",
    "\n",
    "Here, we will reuse `Node` data structure, `completeBinaryTree` and `printTree` functions.  \n",
    "This time, to learn what DFS is, let me show you a simple DFS example.  \n",
    "\n",
    "Consider every route/path from the root (top Node of the tree) to the leaf (bottom Node of the tree). \n",
    "Every step, you should swim down the tree and sum the value of each node.\n",
    "Among the paths, if at least one sum of the path is equal to the number that you were given, you return True and False otherwise.  \n",
    "\n",
    "For example, say you were given a number 10,  \n",
    "and your tree looks like:  \n",
    "```\n",
    "         1\n",
    "     2       3\n",
    "   4   5   6   7\n",
    "```\n",
    "By DFS, you should search every possible route:  \n",
    "1-2-4, 1-2-5, 1-3-6, 1-3-7.  Among these, sum of 1-3-6 make 10, so you return True.   \n",
    "If the given number was 18, you return False, because no single route can make 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth First Search in Binary Tree\n",
    "def path_sum(node, targetSum):\n",
    "    def dfsHelper(node, curSum):\n",
    "\n",
    "        if node is None:\n",
    "            if curSum == targetSum:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            curSum += node.val\n",
    "            is_left = dfsHelper(node.left, curSum)\n",
    "            is_right = dfsHelper(node.right, curSum)\n",
    "\n",
    "        return is_left or is_right\n",
    "\n",
    "    dfsHelper(node, 0)\n",
    "\n",
    "    return dfsHelper(node, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dfs = completeBinaryTree([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])\n",
    "print(printTree(node_dfs))\n",
    "\n",
    "print(path_sum(node_dfs, 16))\n",
    "print(path_sum(node_dfs, 22))\n",
    "print(path_sum(node_dfs, 35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Manually emulating a simple Binary Tree Classifier (optional)\n",
    "Try chaining configurations and code to see what's exactly happpening in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a Classifier that classifies names based on major, height and gender\n",
    "name = ['Nick', 'Danny', 'Claire', 'Rachel', 'Kamen', 'Bianca', 'Alice']\n",
    "major = ['CS', 'CS', 'Neuroscience', 'Neuroscience', 'CS', 'Neuroscience', 'History']\n",
    "height = [180, 177, 163, 168, 182, 170, 165]\n",
    "gender = ['M', 'M', 'F', 'F', 'M', 'F', 'F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node number --> should be a global var\n",
    "num = 0\n",
    "\n",
    "# print as a Dataframe\n",
    "data = pd.DataFrame({'Name': name, 'Major': major, 'Height': height, 'Gender': gender})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Height classifier Node\n",
    "def Height_Node(df, idx, depth):\n",
    "    global num\n",
    "    num += 1\n",
    "    print('Node_num : {} | Node Depth : {} | Height_Node'.format(num, depth))\n",
    "\n",
    "    for i in idx:\n",
    "        num += 1\n",
    "        # Gender probably can have a strong correlation with heights\n",
    "        if df['Gender'][i] == 'M':\n",
    "            # 180 is classifying standard for men\n",
    "            if df['Height'][i] < 180:\n",
    "                print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df['Name'][i]))\n",
    "            else:\n",
    "                print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df['Name'][i]))\n",
    "        \n",
    "        else:\n",
    "            # 165 is classifying standard for women\n",
    "            if df['Height'][i] < 165:\n",
    "                print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df['Name'][i]))\n",
    "            else:\n",
    "                print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df['Name'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major classifier Node\n",
    "def Major_Node(df, idx, depth):\n",
    "    global num\n",
    "    num += 1\n",
    "    print('Node_num : {} | Node Depth : {} | Gender_Node'.format(num, depth))\n",
    "\n",
    "    # List for storing index of CS guys\n",
    "    CS = []\n",
    "\n",
    "    for i in idx:\n",
    "        # Store index if one's major is CS\n",
    "        if df['Major'][i] == 'CS':\n",
    "            CS.append(i)\n",
    "\n",
    "        # print Node number, depth and name if Neuroscience\n",
    "        else:\n",
    "            num += 1\n",
    "            print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df['Name'][i]))\n",
    "\n",
    "    print('CS Index : ', CS)\n",
    "\n",
    "    # Classify CS guys by height, and depth should be increased by 1\n",
    "    Height_Node(df, CS, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender classifier Node\n",
    "def Gender_Node(df, depth):\n",
    "    global num\n",
    "    num += 1\n",
    "    print('Node_num : {} | Node Depth : {} | Gender_Node'.format(num, depth))\n",
    "\n",
    "    male = []\n",
    "    female = []\n",
    "    \n",
    "    # classify by gender\n",
    "    for idx, gen in enumerate(df['Gender']):\n",
    "        if gen == 'M':\n",
    "            male.append(idx)\n",
    "        elif gen == 'F':\n",
    "            female.append(idx)\n",
    "\n",
    "    print('Male Index : ', male)\n",
    "    print('Female Index : ', female)\n",
    "\n",
    "    # Call Major classifier after classifying peeps by gender\n",
    "    # Always increase depth by 1 when calling next node.\n",
    "    # Major Node will then call Height Node\n",
    "    Major_Node(df, male, depth + 1)\n",
    "    Major_Node(df, female, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender_Node(data, 1); num=0  # num=0 is just to reinitialise `num` if you run this cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. Decision Tree\n",
    "Now you're ready to dive into the real Decision Tree Classifier!  \n",
    "Please do take time to watch videos below. They are worth watching. \n",
    "\n",
    "### Must watch: Good video about Decision tree\n",
    "1. Decision Tree Basics: https://www.youtube.com/watch?v=7VeUPuFGJHk&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=38\n",
    "2. Decision Tree Advanced: https://www.youtube.com/watch?v=wpNl-JwwplA&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Always check documentation\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data set\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris is a dictionary that contains data, target data, and other information\n",
    "# so you can access values by specifying key name\n",
    "# for more detailed info, \n",
    "# visit: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X data has its shape of 150x4\n",
    "# y data has its shape of (150,)  --> which means that it is just an array of numbers --> can be reshaped to (150, 1)\n",
    "iris.data.shape, iris.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row has 4 training features, which are ouputted below\n",
    "iris.data[0], iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row the answer is one of three flowers- setosa, versicolor, or virginica\n",
    "iris.target, iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the iris dataset. tes\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get decision tree classifier from scikit learn\n",
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters candidates\n",
    "# max_depth: The maximum depth of the tree. \n",
    "#            If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "# min_samples_split: The minimum number of samples required to split an internal node:\n",
    "#                    If int, then consider min_samples_split as the minimum number.\n",
    "#                    If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "param_grid = {'max_depth': [1, 2, 3], 'min_samples_split': [2, 3]}\n",
    "\n",
    "### Grid Search Cross Validation ###\n",
    "# Since i've set 3 params for max_depth and 2 for min_samples_split, \n",
    "# they will try out 6 combinations of those and pick one combination that let our model perform the best\n",
    "## Question: what does cv parameter do, and why did I set cv=3 ?  ##\n",
    "grid_dtree = GridSearchCV(dtree, param_grid=param_grid, cv=3, refit=True, iid=True)\n",
    "# iid is set to True to avoid DeprecationWarning. You don't need to consider what iid is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "grid_dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results_ is an attribute of GridSearchCV \n",
    "# and it is a dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame.\n",
    "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
    "print(scores_df)\n",
    "scores_df[\n",
    "    ['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try printing out different attributes that GridSearchCV have\n",
    "print('GridSearchCV-> best parameter combination : ', grid_dtree.best_params_)\n",
    "print('GridSearchCV-> best score : {0:.4f}'.format(grid_dtree.best_score_))\n",
    "\n",
    "# Get the best model/estimator\n",
    "estimator = grid_dtree.best_estimator_\n",
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = estimator.predict(X_test)\n",
    "print(f'Test Dataset accuracy : {accuracy_score(y_test, pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's time to exercise by yourself\n",
    "\n",
    "**Let's classify digits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Below are new imports\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore digits Dataset by yourself. \n",
    "# TODO: Try visualising each digit, by using matplotlib or any visualisation tool you may prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_digit(index):\n",
    "    # Can be challenging but you should try!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = digits.data\n",
    "label = digits.target\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {'max_depth': np.arange(1, 30), 'min_samples_split': np.arange(2, 7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will going to use KFold CV, just to revise\n",
    "n_splits = 5    # This is 5 for reason, but feel free to try out different numbers\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "n_iter = 0\n",
    "cv_accuracy = []\n",
    "\n",
    "# TODO: Split with ratio 7:3\n",
    "X_train, X_test, y_train, y_test = \"\"\"train test split should be familiar to you now\"\"\"\n",
    "\n",
    "# TODO: Do `n_splits` KFold cross validation\n",
    "for train_idx, vali_idx in kfold.split(\"\"\"with which splitted dataset should you be doing kfold?\"\"\"):\n",
    "    X_fold_train, X_fold_vali = None, None\n",
    "    y_fold_train, y_fold_vali = None, None\n",
    "\n",
    "    # TODO: Train your model with GridSearchCV\n",
    "    grid_dtree = GridSearchCV(\"\"\"Fill in parmas\"\"\", iid=True)\n",
    "    grid_dtree.fit(None)\n",
    "\n",
    "    # TODO: predict the output for each fold with validation set\n",
    "    fold_pred = grid_dtree.predict(None)\n",
    "\n",
    "    n_iter += 1\n",
    "    acc_score = accuracy_score(y_fold_vali, fold_pred)\n",
    "    print('Iter : {0}, CV accuary : {1:.4f}'.format(n_iter, acc_score))\n",
    "\n",
    "    cv_accuracy.append(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate the mean of cv accuracy\n",
    "cv_accuracy_mean = None\n",
    "print(\"Mean acc : {0:.4f}\".format(cv_accuracy_mean))\n",
    "\n",
    "# TODO: What's the best param combi and best score?\n",
    "print('GridSearchCV -> best param : ', None)\n",
    "print('GridSearchCV -> best accuracy : {0:.4f}'.format(None))\n",
    "\n",
    "# TODO: get the best performing model\n",
    "estimator = None\n",
    "\n",
    "# TODO: predict with test data\n",
    "pred = estimator.predict(None)\n",
    "\n",
    "# How's the accuarcy of unseen data?\n",
    "test_accuracy = accuracy_score(pred, y_test)\n",
    "print(\"test accuarcy : {0:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who want to learn further, learn:\n",
    "- KNeighbors Classifier\n",
    "- Ensemble\n",
    "- Voting Classifier\n",
    "- Gradient Boosting\n",
    "- Random Forest\n",
    "\n",
    "Some of those above can be dealt in the next session :) so stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
